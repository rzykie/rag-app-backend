# render.yaml
# This file defines the services for the RAG API application on Render.

services:
  # 1. Redis service for Celery message brokering
  - name: redis
    type: redis
    plan: starter

  # 2. The private service for running the Ollama server
  - name: ollama-server
    type: private
    env: docker
    dockerfilePath: ./ollama.Dockerfile
    # Give the service more resources, as LLMs are demanding
    plan: standard
    # Attach a persistent disk to store the downloaded models
    disks:
      - name: ollama-models
        mountPath: /root/.ollama
        sizeGB: 20
    # The script to run on startup to pull your desired model
    startCommand: ./start-ollama.sh

  # 3. The Celery worker for background data ingestion
  - name: worker
    type: worker
    env: docker
    dockerfilePath: ./Dockerfile
    plan: starter
    # Command to start the worker and trigger the initial ingestion task
    startCommand: celery -A celery_app.celery call process_documents && celery -A celery_app.celery worker --loglevel=info
    envVars:
      - key: OLLAMA_BASE_URL
        value: http://ollama-server:11434
      - key: CELERY_BROKER_URL
        fromService:
          type: redis
          name: redis
          property: connectionString

  # 4. The public web service for your FastAPI application
  - name: api
    type: web
    env: docker
    # We use the existing Dockerfile for the API
    dockerfilePath: ./Dockerfile
    plan: starter # Can be starter, will scale automatically
    ports:
      - "8000:8000"
    envVars:
      # This is the crucial part: we point our API to the private Ollama service
      # Render provides internal DNS, so we can use the service name 'ollama-server'.
      - key: OLLAMA_BASE_URL
        value: http://ollama-server:11434
      - key: CELERY_BROKER_URL
        fromService:
          type: redis
          name: redis
          property: connectionString
      # Ensure python buffering is disabled for real-time logging
      - key: PYTHONUNBUFFERED
        value: 1
